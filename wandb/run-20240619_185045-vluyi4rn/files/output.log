Using cuda device
Eval num_timesteps=1000, episode_reward=105.30 +/- 9.01
Episode length: 132.60 +/- 9.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 133      |
|    mean_reward     | 105      |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
New best mean reward!
Eval num_timesteps=2000, episode_reward=106.65 +/- 3.96
Episode length: 133.20 +/- 3.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 133      |
|    mean_reward     | 107      |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.5     |
|    ep_rew_mean     | 13.1     |
| time/              |          |
|    fps             | 551      |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 2048     |
---------------------------------
Eval num_timesteps=3000, episode_reward=78.52 +/- 1.15
Episode length: 50.00 +/- 0.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 78.5        |
| time/                   |             |
|    total_timesteps      | 3000        |
| train/                  |             |
|    approx_kl            | 0.013354601 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.05        |
|    learning_rate        | 0.0003      |
|    loss                 | 6.09        |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0208     |
|    std                  | 0.988       |
|    value_loss           | 24          |
-----------------------------------------
Eval num_timesteps=4000, episode_reward=79.70 +/- 0.82
Episode length: 50.60 +/- 0.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | 79.7     |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.8     |
|    ep_rew_mean     | 22.9     |
| time/              |          |
|    fps             | 541      |
|    iterations      | 2        |
|    time_elapsed    | 7        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=67.39 +/- 0.75
Episode length: 41.20 +/- 0.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 41.2       |
|    mean_reward          | 67.4       |
| time/                   |            |
|    total_timesteps      | 5000       |
| train/                  |            |
|    approx_kl            | 0.01460869 |
|    clip_fraction        | 0.126      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.22      |
|    explained_variance   | 0.277      |
|    learning_rate        | 0.0003     |
|    loss                 | 29.1       |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.0208    |
|    std                  | 0.985      |
|    value_loss           | 106        |
----------------------------------------
Eval num_timesteps=6000, episode_reward=67.61 +/- 1.40
Episode length: 41.40 +/- 0.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.4     |
|    mean_reward     | 67.6     |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33       |
|    ep_rew_mean     | 42.1     |
| time/              |          |
|    fps             | 533      |
|    iterations      | 3        |
|    time_elapsed    | 11       |
|    total_timesteps | 6144     |
---------------------------------
|    entropy_loss         | -4.21      |63.74 +/- 1.13
Episode length: 39.00 +/- 0.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 39         |
|    mean_reward          | 63.7       |
| time/                   |            |
|    total_timesteps      | 7000       |
| train/                  |            |
|    approx_kl            | 0.00976699 |
|    clip_fraction        | 0.0916     |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.21      |63.74 +/- 1.13
|    explained_variance   | 0.216      |
|    learning_rate        | 0.0003     |
|    loss                 | 78.4       |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.0165    |
|    std                  | 0.982      |
|    value_loss           | 207        |
----------------------------------------
Eval num_timesteps=8000, episode_reward=64.20 +/- 0.80
Episode length: 39.20 +/- 0.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39.2     |
|    mean_reward     | 64.2     |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 44       |
|    ep_rew_mean     | 65.4     |
| time/              |          |
|    fps             | 531      |
|    iterations      | 4        |
|    time_elapsed    | 15       |
|    total_timesteps | 8192     |
---------------------------------
[35m   0%[39m [38m━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[39m [32m8,540/2,000,000 [39m [ [33m0:00:17[39m < [36m1:04:06[39m , [31m518 it/s[39m ]
[?25h------------------------------------
| eval/                   |             |
|    mean_ep_length       | 39.4        |
|    mean_reward          | 64.5        |
| time/                   |             |
|    total_timesteps      | 9000        |
| train/                  |             |
|    approx_kl            | 0.008668717 |
|    clip_fraction        | 0.0588      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.0614      |
|    learning_rate        | 0.0003      |
|    loss                 | 90.9        |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0126     |
|    std                  | 0.982       |
|    value_loss           | 243         |
-----------------------------------------
Stopping training because there was no new best model in the last 4 evaluations
[35m   0%[39m [38m━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[39m [32m9,000/2,000,000 [39m [ [33m0:00:17[39m < [36m1:03:09[39m , [31m526 it/s[39m ]
[?25h------------------------------------
Traceback (most recent call last):
  File "/home/ale/TT_RLProject_2024/Task_6/UDR_Training.py", line 80, in <module>
    main()
  File "/home/ale/TT_RLProject_2024/Task_6/UDR_Training.py", line 60, in main
    model_name = model_name.replace('.', '_')
UnboundLocalError: local variable 'model_name' referenced before assignment