Using cuda device
Eval num_timesteps=8000, episode_reward=93.63 +/- 0.60
Episode length: 111.40 +/- 1.36
New best mean reward!
Eval num_timesteps=16000, episode_reward=100.49 +/- 4.63
Episode length: 117.40 +/- 3.56
New best mean reward!
Eval num_timesteps=24000, episode_reward=94.74 +/- 8.82
Episode length: 111.60 +/- 8.31
Eval num_timesteps=32000, episode_reward=93.03 +/- 5.98
Episode length: 110.40 +/- 6.47










Eval num_timesteps=40000, episode_reward=134.01 +/- 1.71
Episode length: 73.40 +/- 0.80
New best mean reward!
Eval num_timesteps=48000, episode_reward=134.16 +/- 1.63
Episode length: 73.40 +/- 0.80
New best mean reward!
Eval num_timesteps=56000, episode_reward=133.42 +/- 1.23
Episode length: 73.00 +/- 0.63
Eval num_timesteps=64000, episode_reward=133.40 +/- 1.31
Episode length: 73.00 +/- 0.63









Eval num_timesteps=72000, episode_reward=133.73 +/- 0.88
Episode length: 67.60 +/- 0.49
Eval num_timesteps=80000, episode_reward=133.01 +/- 0.72
Episode length: 67.20 +/- 0.40
Eval num_timesteps=88000, episode_reward=133.01 +/- 0.73
Episode length: 67.20 +/- 0.40
Eval num_timesteps=96000, episode_reward=133.44 +/- 1.92
Episode length: 67.40 +/- 1.02











Eval num_timesteps=104000, episode_reward=215.60 +/- 1.03
Episode length: 95.20 +/- 0.40
New best mean reward!
Eval num_timesteps=112000, episode_reward=215.28 +/- 1.45
Episode length: 95.00 +/- 0.63
Eval num_timesteps=120000, episode_reward=214.85 +/- 0.95
Episode length: 94.80 +/- 0.40
Eval num_timesteps=128000, episode_reward=215.52 +/- 0.19
Episode length: 95.00 +/- 0.00

[35m 100%[39m [38m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[39m [32m131,072/100,000 [39m [ [33m0:01:18[39m < [36m0:00:00[39m , [31m1,534 it/s[39m ]
[?25h